from fastapi import APIRouter, Body, HTTPException, status
from pydantic import BaseModel, constr
from typing import List, Optional
import logging
from main import query_local_llm

router = APIRouter()

MAX_TEXT_LENGTH = 100_000  # Same as review step

class GenerateTutorQuestionsRequest(BaseModel):
    reviewed_text: constr(min_length=1, max_length=MAX_TEXT_LENGTH)
    filename: Optional[str] = None
    filetype: Optional[str] = None

class TutorQuestion(BaseModel):
    type: str  # "open" or "mcq"
    question: Optional[str] = None  # For open questions
    stem: Optional[str] = None  # For MCQ questions
    options: Optional[List[str]] = None  # For MCQ options
    answer: str
    explanation: Optional[str] = None  # For MCQ explanation
    difficulty: str
    hint: Optional[str] = None
    reference: Optional[str] = None
    tags: List[str]

class GenerateTutorQuestionsResponse(BaseModel):
    success: bool
    learning_objectives: List[str]
    questions: List[TutorQuestion]
    message: str

def parse_llm_questions(llm_output) -> tuple[List[str], List[TutorQuestion]]:
    """Parse LLM output into learning objectives and questions"""
    learning_objectives = llm_output.get("learning_objectives", [])
    questions = []
    
    for item in llm_output.get("tutor_questions", []):
        difficulty = item.get("difficulty")
        tags = item.get("tags", [])
        if not difficulty or not tags:
            continue  # Skip incomplete questions
            
        if item.get("type", "open") == "open":
            questions.append(
                TutorQuestion(
                    type="open",
                    question=item.get("question", ""),
                    answer=item.get("answer", ""),
                    reference=item.get("reference"),
                    difficulty=difficulty,
                    hint=item.get("hint"),
                    tags=tags,
                )
            )
        elif item.get("type", "") == "mcq":
            stem = item.get("question", "")  # MCQ questions use "question" field for stem
            options = item.get("options", [])
            answer = item.get("answer", "")
            explanation = item.get("explanation", "")
            ref = item.get("reference")
            hint = item.get("hint")
            mcq_tags = item.get("tags", [])
            
            if stem and options and answer and len(options) >= 4:
                questions.append(
                    TutorQuestion(
                        type="mcq",
                        stem=stem,
                        options=options,
                        answer=answer,
                        explanation=explanation,
                        reference=ref,
                        difficulty=difficulty,
                        hint=hint,
                        tags=mcq_tags,
                    )
                )
    
    return learning_objectives, questions

@router.post("/generate-tutor-questions", response_model=GenerateTutorQuestionsResponse, status_code=status.HTTP_200_OK)
async def generate_tutor_questions(request: GenerateTutorQuestionsRequest = Body(...)):
    """Generate tutor questions from reviewed text using LLM"""
    text = request.reviewed_text.strip()
    if not text:
        raise HTTPException(status_code=422, detail="Reviewed text is empty.")
    if len(text) > MAX_TEXT_LENGTH:
        raise HTTPException(status_code=413, detail=f"Text too long. Max allowed is {MAX_TEXT_LENGTH} characters.")

    try:
        llm_output = query_local_llm(text)
        learning_objectives, questions = parse_llm_questions(llm_output)
    except Exception as e:
        logging.error(f"Error during LLM question generation: {e}")
        raise HTTPException(status_code=500, detail=f"Error during question generation: {str(e)}")

    if not questions:
        raise HTTPException(status_code=204, detail="No tutor questions were generated by the AI.")

    return GenerateTutorQuestionsResponse(
        success=True,
        learning_objectives=learning_objectives,
        questions=questions,
        message="Tutor questions generated successfully."
    )

# Alternative endpoint that matches the frontend expectation
@router.post("/tutor-questions")
async def tutor_questions_endpoint(request: dict = Body(...)):
    """Alternative endpoint for tutor questions that matches frontend expectations"""
    content = request.get("content", "")
    
    if not content:
        # Return empty but valid response for missing content
        return {
            "learning_objectives": [],
            "tutor_questions": []
        }
    
    try:
        llm_output = query_local_llm(content)
        return llm_output
    except Exception as e:
        logging.error(f"Error in tutor-questions endpoint: {e}")
        raise HTTPException(status_code=500, detail=f"Error generating questions: {str(e)}")