from fastapi import APIRouter, Body, HTTPException, status
from pydantic import BaseModel, constr
from typing import List, Optional
import logging
from python_worker.main import query_local_llm

router = APIRouter()

MAX_TEXT_LENGTH = 100_000  # Same as review step

class GenerateTutorQuestionsRequest(BaseModel):
    reviewed_text: constr(min_length=1, max_length=MAX_TEXT_LENGTH)
    filename: Optional[str] = None
    filetype: Optional[str] = None

class TutorQuestion(BaseModel):
    question: str
    answer: str
    reference: Optional[str] = None
    difficulty: Optional[str] = None
    hint: Optional[str] = None

class GenerateTutorQuestionsResponse(BaseModel):
    success: bool
    questions: List[TutorQuestion]
    message: Optional[str] = None

def parse_llm_questions(llm_output) -> List[TutorQuestion]:
    questions = []
    for item in llm_output.get("tutor_questions", []):
        if item.get("type", "open") == "open":
            # Reasoning-focused open question
            questions.append(
                TutorQuestion(
                    question=item.get("question", ""),
                    answer=item.get("answer", ""),
                    reference=item.get("reference"),
                    difficulty=item.get("difficulty"),
                    hint=item.get("hint"),
                )
            )
        elif item.get("type", "") == "mcq":
            # MCQ as fallback/secondary
            stem = item.get("stem", "")
            options = item.get("options", [])
            answer = item.get("answer", "")
            explanation = item.get("explanation", "")
            ref = item.get("reference")
            difficulty = item.get("difficulty")
            hint = item.get("hint")
            mcq_question = f"MCQ: {stem}\nOptions: {', '.join(options)}\nAnswer: {answer}\nExplanation: {explanation}"
            questions.append(
                TutorQuestion(
                    question=mcq_question,
                    answer=answer,
                    reference=ref,
                    difficulty=difficulty,
                    hint=hint,
                )
            )
    return questions

@router.post("/generate-tutor-questions", response_model=GenerateTutorQuestionsResponse, status_code=status.HTTP_200_OK)
async def generate_tutor_questions(request: GenerateTutorQuestionsRequest = Body(...)):
    text = request.reviewed_text.strip()
    if not text:
        raise HTTPException(status_code=422, detail="Reviewed text is empty.")
    if len(text) > MAX_TEXT_LENGTH:
        raise HTTPException(status_code=413, detail=f"Text too long. Max allowed is {MAX_TEXT_LENGTH} characters.")

    try:
        llm_output = query_local_llm(text)
        questions = parse_llm_questions(llm_output)
    except Exception as e:
        logging.error(f"Error during LLM question generation: {e}")
        raise HTTPException(status_code=500, detail=f"Error during question generation: {str(e)}")

    if not questions:
        raise HTTPException(status_code=204, detail="No tutor questions were generated by the AI.")

    return GenerateTutorQuestionsResponse(
        success=True,
        questions=questions,
        message="Tutor questions generated successfully."
    )
